{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "class Batch:\n",
    "    def __init__(self, data, batchSize):\n",
    "        self.data = np.copy(data)\n",
    "        self.count = 0\n",
    "        self.batchSize = batchSize\n",
    "    \n",
    "    def getNextBatch(self):\n",
    "        if self.count >= len(self.data) / self.batchSize:\n",
    "            self.count = 0\n",
    "            np.random.shuffle(self.data)\n",
    "        start = self.count * self.batchSize\n",
    "        end = start + self.batchSize\n",
    "        toReturn = self.data[start:end]\n",
    "        self.count += 1\n",
    "        return toReturn\n",
    "\n",
    "LEARNING_RATE_START = 0.0001\n",
    "LEARNING_RATE_END   = 0.000001\n",
    "N_EPOCHS      = 500\n",
    "anneal_rate = (-1.0 * np.log(LEARNING_RATE_END / LEARNING_RATE_START)) / float(N_EPOCHS)\n",
    "batchSize = 50\n",
    "\n",
    "train_data = np.load(\"../data/trainingDataClassify.npy\")\n",
    "valid_data = np.load(\"../data/validationDataClassify.npy\")\n",
    "print(len(train_data))\n",
    "trainBatch = Batch(train_data, batchSize)\n",
    "epochSize = len(train_data) / batchSize\n",
    "curr_lr = LEARNING_RATE_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "beta = 0.001\n",
    "A = tf.Variable(tf.random_normal([60, 2048], stddev=0.1))\n",
    "B = tf.Variable(tf.random_normal([2048, 2048], stddev=0.1))\n",
    "C = tf.Variable(tf.random_normal([2048, 1024], stddev=0.1)) \n",
    "D = tf.Variable(tf.random_normal([1024, 1024], stddev=0.1))\n",
    "E = tf.Variable(tf.random_normal([1024, 2780], stddev=0.1))\n",
    "biasA = tf.Variable(tf.zeros([2048]), name = 'biasesA')\n",
    "biasB = tf.Variable(tf.zeros([2048]), name = 'biasesB')\n",
    "biasC = tf.Variable(tf.zeros([1024]), name = 'biasesC')\n",
    "biasD = tf.Variable(tf.zeros([1024]), name = 'biasesD')\n",
    "biasE = tf.Variable(tf.zeros([2780]), name = 'biasesE')\n",
    "X = tf.placeholder(tf.float32, [None, 60])\n",
    "y = tf.placeholder(tf.float32, [None, 2780]) \n",
    "lr  = tf.placeholder(tf.float32)\n",
    "\n",
    "regularizer = tf.nn.l2_loss(A) + tf.nn.l2_loss(B) + tf.nn.l2_loss(C) + tf.nn.l2_loss(D)\n",
    "eval_dict={\n",
    "X   : valid_data[:,:-1],\n",
    "y   : valid_data[:,-1:]\n",
    "}\n",
    "\n",
    "train_dict = {\n",
    "    X : train_data[:,:-1],\n",
    "    y : train_data[:,-1:]\n",
    "}\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.matmul(tf.cast(X, tf.float32), A) + biasA)\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, B) + biasB)\n",
    "hidden3 = tf.nn.relu(tf.matmul(hidden2, C) + biasC)\n",
    "hidden4 = tf.nn.relu(tf.matmul(hidden3, D) + biasD)\n",
    "estimate = tf.nn.softmax(tf.matmul(hidden4, E) + biasE)\n",
    "#estimate = tf.clip_by_value(estimate, 0, 2779)\n",
    "\n",
    "metric = tf.reduce_mean(tf.abs(tf.transpose(estimate) - y))\n",
    "loss = tf.nn.l2_loss(estimate - y)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver([A,B,C,D,E,biasA, biasB, biasC, biasD, biasE])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# saver = tf.train.import_meta_graph('../weights/ZestimateWeight.meta')\n",
    "# saver.restore(sess, tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_loss = sess.run(loss, eval_dict)\n",
    "best_training_loss = None\n",
    "for i in range(N_EPOCHS*epochSize):\n",
    "    checkpointed = False\n",
    "    \n",
    "    if i %  epochSize == 0:\n",
    "        print(\"epoch #\" + str(i /epochSize))\n",
    "        valid_loss = sess.run(loss, eval_dict)\n",
    "        train_loss = sess.run(loss, train_dict)\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_training_loss = train_loss\n",
    "            saver.save(sess, \"../weights/ZestimateWeight\")\n",
    "            if i >= 25*epochSize:  \n",
    "                curr_lr = LEARNING_RATE_START * np.exp(-1.0 * anneal_rate * i)\n",
    "            checkpointed = True\n",
    "        print(\"checkPointed = \" + str(checkpointed))\n",
    "        print(\"validationLoss = \" + str(valid_loss))\n",
    "        print(\"trainLoss = \" + str(train_loss))\n",
    "    currBatch = trainBatch.getNextBatch()\n",
    "    sess.run(train_step, feed_dict={\n",
    "       X   : currBatch[:, :-1],\n",
    "       y   : currBatch[:, -1:],\n",
    "       lr  : curr_lr\n",
    "    })\n",
    "print(\"BEST LOSS = \" + str(best_loss))  \n",
    "print(\"Corresponding Train Loss = \" + str(best_training_loss)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preds = sess.run(estimate, { X: testData[:, 1:]})\n",
    "#np.save(\"NNPredictionsV2_\" + str(i) + \".npy\", preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
